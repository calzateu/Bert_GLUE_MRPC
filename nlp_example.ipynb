{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find / -name nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# This is a fully working simple example to use Accelerate\n",
    "#\n",
    "# This example trains a Bert base model on GLUE MRPC\n",
    "# in any of the following settings (with the same script):\n",
    "#   - single CPU or single GPU\n",
    "#   - multi GPUS (using PyTorch distributed mode)\n",
    "#   - (multi) TPUs\n",
    "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "#\n",
    "# To run it in each of these various modes, follow the instructions\n",
    "# in the readme for examples:\n",
    "# https://github.com/huggingface/accelerate/tree/main/examples\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "\n",
    "MAX_GPU_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def training_function(config, args):\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(fp16=args.fp16, cpu=args.cpu)\n",
    "\n",
    "    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n",
    "    lr = config[\"lr\"]\n",
    "    num_epochs = int(config[\"num_epochs\"])\n",
    "    correct_bias = config[\"correct_bias\"]\n",
    "    seed = int(config[\"seed\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # max_length=None => use the model max length (it's actually the default)\n",
    "        outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=None)\n",
    "        return outputs\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    "    )\n",
    "\n",
    "    # We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n",
    "    # transformers library\n",
    "    tokenized_datasets.rename_column_(\"label\", \"labels\")\n",
    "\n",
    "    # If the batch size is too big we use gradient accumulation\n",
    "    gradient_accumulation_steps = 1\n",
    "    if batch_size > MAX_GPU_BATCH_SIZE:\n",
    "        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n",
    "        batch_size = MAX_GPU_BATCH_SIZE\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
    "        if accelerator.distributed_type == DistributedType.TPU:\n",
    "            return tokenizer.pad(examples, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    # Instantiate dataloaders.\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=EVAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
    "\n",
    "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
    "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
    "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
    "    model = model.to(accelerator.device)\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    optimizer = AdamW(params=model.parameters(), lr=lr, correct_bias=correct_bias)\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n",
    "    # may change its length.\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=len(train_dataloader) * num_epochs,\n",
    "    )\n",
    "\n",
    "    # Now we train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            batch.to(accelerator.device)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            batch.to(accelerator.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            metric.add_batch(\n",
    "                predictions=accelerator.gather(predictions),\n",
    "                references=accelerator.gather(batch[\"labels\"]),\n",
    "            )\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"If passed, will use FP16 training.\")\n",
    "    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
    "    args = parser.parse_args()\n",
    "    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"correct_bias\": True, \"seed\": 42, \"batch_size\": 16}\n",
    "    training_function(config, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
