{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "nlp_example.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR3PChriIRwi",
        "outputId": "45ae6f35-edcc-4615-f843-e88a98031e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install datasets transformers\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 20.9 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.0-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 28.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 33.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 53.7 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 datasets-1.15.1 frozenlist-1.2.0 fsspec-2021.10.1 huggingface-hub-0.1.0 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.2 xxhash-2.0.2 yarl-1.7.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74au-wQXI_iR",
        "outputId": "31f322a3-4d01-4e34-cc4f-d810c60183cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install accelerate"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 20 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 40 kB 35.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 51 kB 38.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beFqR__UIRwj",
        "outputId": "4925872d-0a18-4b41-fe65-5a22ee79a3e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!find / -name nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/nvidia-smi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79onrFlWIRwk",
        "outputId": "72156454-4bfa-44b0-b36f-addbad9966cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/usr/bin/nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnhX1MBUIRwk",
        "outputId": "65d39b5a-3e3c-4e7f-dae8-025defbbd4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from accelerate import Accelerator, DistributedType\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# This is a fully working simple example to use Accelerate\n",
        "#\n",
        "# This example trains a Bert base model on GLUE MRPC\n",
        "# in any of the following settings (with the same script):\n",
        "#   - single CPU or single GPU\n",
        "#   - multi GPUS (using PyTorch distributed mode)\n",
        "#   - (multi) TPUs\n",
        "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
        "#\n",
        "# To run it in each of these various modes, follow the instructions\n",
        "# in the readme for examples:\n",
        "# https://github.com/huggingface/accelerate/tree/main/examples\n",
        "#\n",
        "########################################################################\n",
        "\n",
        "\n",
        "MAX_GPU_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "def training_function(config, args):\n",
        "    # Initialize accelerator\n",
        "    accelerator = Accelerator(fp16=args.fp16, cpu=args.cpu)\n",
        "\n",
        "    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n",
        "    lr = config[\"lr\"]\n",
        "    num_epochs = int(config[\"num_epochs\"])\n",
        "    correct_bias = config[\"correct_bias\"]\n",
        "    seed = int(config[\"seed\"])\n",
        "    batch_size = int(config[\"batch_size\"])\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "    metric = load_metric(\"glue\", \"mrpc\")\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        # max_length=None => use the model max length (it's actually the default)\n",
        "        outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=None)\n",
        "        return outputs\n",
        "\n",
        "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
        "    tokenized_datasets = datasets.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
        "    )\n",
        "\n",
        "    # We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n",
        "    # transformers library\n",
        "    tokenized_datasets.rename_column_(\"label\", \"labels\")\n",
        "\n",
        "    # If the batch size is too big we use gradient accumulation\n",
        "    gradient_accumulation_steps = 1\n",
        "    if batch_size > MAX_GPU_BATCH_SIZE:\n",
        "        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n",
        "        batch_size = MAX_GPU_BATCH_SIZE\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
        "        if accelerator.distributed_type == DistributedType.TPU:\n",
        "            return tokenizer.pad(examples, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
        "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
        "\n",
        "    # Instantiate dataloaders.\n",
        "    train_dataloader = DataLoader(\n",
        "        tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=EVAL_BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
        "\n",
        "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
        "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
        "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
        "    model = model.to(accelerator.device)\n",
        "\n",
        "    # Instantiate optimizer\n",
        "    optimizer = AdamW(params=model.parameters(), lr=lr, correct_bias=correct_bias)\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
        "    # prepare method.\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader\n",
        "    )\n",
        "\n",
        "    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n",
        "    # may change its length.\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=100,\n",
        "        num_training_steps=len(train_dataloader) * num_epochs,\n",
        "    )\n",
        "\n",
        "    # Now we train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
        "            batch.to(accelerator.device)\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            accelerator.backward(loss)\n",
        "            if step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
        "            batch.to(accelerator.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "            predictions = outputs.logits.argmax(dim=-1)\n",
        "            metric.add_batch(\n",
        "                predictions=accelerator.gather(predictions),\n",
        "                references=accelerator.gather(batch[\"labels\"]),\n",
        "            )\n",
        "\n",
        "        eval_metric = metric.compute()\n",
        "        # Use accelerator.print to print only on the main process.\n",
        "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
        "    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"If passed, will use FP16 training.\")\n",
        "    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
        "    args = parser.parse_args()\n",
        "    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"correct_bias\": True, \"seed\": 42, \"batch_size\": 16}\n",
        "    training_function(config, args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--fp16] [--cpu]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-afdf4a44-3105-4dfe-8ac6-6c51ad86401b.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}